import{_ as e,c as s,e as a,o as n}from"./app-Br-LFPki.js";const o={};function r(d,t){return n(),s("div",null,[...t[0]||(t[0]=[a(`<h2 id="版本对比与推荐" tabindex="-1"><a class="header-anchor" href="#版本对比与推荐"><span>版本对比与推荐</span></a></h2><table><thead><tr><th><strong>项目</strong></th><th>Kettle 8.3.0.0-371</th><th>Kettle 9.3.0.0-428</th></tr></thead><tbody><tr><td><strong>最低 JDK</strong></td><td>JDK 1.8(Java 8)</td><td>JDK 1.8</td></tr><tr><td><strong>推荐 JDK</strong></td><td>JDK 1.8(Java 8)</td><td><strong>JDK 11</strong></td></tr><tr><td><strong>配置重点</strong></td><td>避免 JDK 1.7 冲突</td><td>推荐 JDK 11 以兼容新特性（如 Hadoop 3.x 集成、Kafka连接器等）</td></tr></tbody></table><ul><li><code>Kettle 8.3.0.0-371​</code>: 传统数据库ETL场景​</li><li><code>Kettle 9.3.0.0-428​</code>(pdi-ce-9.3.0.0-428.zip): a. Hadoop/Spark集成场景​: 1. 原生支持Hive 3.x/Spark 3.x实时流. 2. 支持AEL引擎（任务下压Spark集群加速）b.云原生&amp;实时处理场景: 1. 支持容器化部署（Docker/K8s） 2. 集成Kafka/MongoDB实时数据流 3. 适配AWS EMR/Google Dataproc</li></ul><p>新项目或需大数据集成 → 选 <code>Kettle 9.3</code> + <code>JDK 11</code></p><h3 id="kettle-9-3和shims驱动包" tabindex="-1"><a class="header-anchor" href="#kettle-9-3和shims驱动包"><span>Kettle 9.3和Shims驱动包</span></a></h3><p>若使用 <code>Kettle 9.3.0.0-428</code> 仅涉及基础功能（传统数据库ETL），无需额外安装 Shims 驱动包。</p><hr><h3 id="✅-一、无需-shims-驱动包的场景-基础功能" tabindex="-1"><a class="header-anchor" href="#✅-一、无需-shims-驱动包的场景-基础功能"><span>✅ 一、无需 Shims 驱动包的场景（基础功能）</span></a></h3><h4 id="支持的操作-仅依赖-kettle-默认驱动" tabindex="-1"><a class="header-anchor" href="#支持的操作-仅依赖-kettle-默认驱动"><span><strong>支持的操作（仅依赖 Kettle 默认驱动）</strong></span></a></h4><table><thead><tr><th><strong>功能类型</strong></th><th><strong>具体组件</strong></th></tr></thead><tbody><tr><td><strong>关系型数据库</strong></td><td>MySQL/Oracle/PostgreSQL/SQL Server/DB2</td></tr><tr><td><strong>文件处理</strong></td><td>CSV/Excel/TXT 读写、ZIP压缩</td></tr><tr><td><strong>基础转换步骤</strong></td><td>字段选择、排序、去重、计算器、数据校验</td></tr><tr><td><strong>作业控制</strong></td><td>定时调度、邮件通知、文件检查、Shell脚本</td></tr></tbody></table><blockquote><p>⚠️ 注意：连接 <strong>MySQL 8.x/Oracle 19c</strong> 等新版数据库时，需手动将对应JDBC驱动（如 <code>mysql-connector-java-8.0.30.jar</code>）放入 <code>lib</code> 目录，但这<strong>不属于 Shims 范畴</strong>。</p></blockquote><hr><h3 id="⚠️-二、必须-shims-驱动包的功能-大数据-云服务集成" tabindex="-1"><a class="header-anchor" href="#⚠️-二、必须-shims-驱动包的功能-大数据-云服务集成"><span>⚠️ <strong>二、必须 Shims 驱动包的功能（大数据/云服务集成）</strong></span></a></h3><p>当涉及以下场景时，<strong>必须安装 Shims 驱动包</strong>：</p><table><thead><tr><th><strong>功能类型</strong></th><th>依赖 Shims 的具体组件</th><th>缺失后果</th></tr></thead><tbody><tr><td><strong>Hadoop 生态</strong></td><td>HDFS 读写、Hive 输入/输出、Spark 提交</td><td>作业报错：<code>No Shim driver available</code></td></tr><tr><td><strong>云存储</strong></td><td>AWS S3、Azure Blob、Google Cloud Storage</td><td>无法连接云存储桶</td></tr><tr><td><strong>NoSQL 数据库</strong></td><td>MongoDB、Cassandra、HBase</td><td>转换步骤灰色不可用</td></tr><tr><td><strong>流处理</strong></td><td>Kafka 生产/消费、RabbitMQ 消息队列</td><td>连接器初始化失败</td></tr><tr><td><strong>大数据文件格式</strong></td><td>Parquet、Avro、ORC 文件处理</td><td>文件解析异常</td></tr></tbody></table><blockquote><p>💡 关键识别：若转换中用到 <strong>“Hadoop File Input/Output”</strong> 或 <strong>“Kafka Consumer”</strong> 等步骤，则必须 Shims。</p></blockquote><hr><h3 id="🔧-三、shims-驱动包的最小化安装方案" tabindex="-1"><a class="header-anchor" href="#🔧-三、shims-驱动包的最小化安装方案"><span>🔧 <strong>三、Shims 驱动包的最小化安装方案</strong></span></a></h3><p>若未来可能扩展功能，但当前只需基础ETL，可<strong>仅保留核心驱动</strong>：</p><ol><li>下载官方包：<code>pentaho-shims-9.3.0.0-428.zip</code></li><li><strong>仅解压以下必要组件</strong>到 <code>plugins/pentaho-big-data-plugin</code>：<div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 核心层（必选）</span></span>
<span class="line">pentaho-hadoop-shims-common-9.3.0.0-428.jar</span>
<span class="line">pentaho-hadoop-shims-api-9.3.0.0-428.jar</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 云存储支持（按需）</span></span>
<span class="line"><span class="token comment"># pentaho-hadoop-shims-s3-9.3.0.0-428.jar       # AWS S3</span></span>
<span class="line"><span class="token comment"># pentaho-hadoop-shims-azure-9.3.0.0-428.jar    # Azure Blob   </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li>删除其他无用驱动（如 <code>cdh6*</code>, <code>mapr*</code> 等集群专用包），减少冲突风险。</li></ol><hr><h3 id="🚫-四、常见误区-易混淆的非-shims-依赖" tabindex="-1"><a class="header-anchor" href="#🚫-四、常见误区-易混淆的非-shims-依赖"><span>🚫 <strong>四、常见误区：易混淆的非 Shims 依赖</strong></span></a></h3><p>以下问题<strong>与 Shims 无关</strong>，需单独处理：</p><table><thead><tr><th><strong>报错现象</strong></th><th><strong>真实原因</strong></th><th><strong>解决方案</strong></th></tr></thead><tbody><tr><td><code>No suitable driver found for jdbc:mysql://...</code></td><td>MySQL 驱动未安装</td><td>下载 <code>mysql-connector-java-8.0.30.jar</code> 放入 <code>lib</code></td></tr><tr><td><code>Oracle Thin Client missing</code></td><td>Oracle 驱动缺失</td><td>将 <code>ojdbc8.jar</code> 加入 <code>lib</code></td></tr><tr><td><code>Error parsing XML</code></td><td>XML 步骤需 xercesImpl 库</td><td>添加 <code>xercesImpl-2.12.0.jar</code></td></tr></tbody></table><hr><h3 id="💎-终极建议" tabindex="-1"><a class="header-anchor" href="#💎-终极建议"><span>💎 <strong>终极建议</strong></span></a></h3><ol><li><strong>纯基础ETL场景</strong>： <ul><li>直接使用纯净版 Kettle 9.3.0.0-428，<strong>无需 Shims 包</strong>。</li><li>仅需补充所需数据库驱动（MySQL/Oracle等）到 <code>lib</code> 目录。</li></ul></li><li><strong>预留扩展可能性</strong>： <ul><li>最小化安装 Shims 核心组件（保留 <code>common</code> + <code>api</code> + 云驱动），占用 &lt;10MB 空间。</li></ul></li><li><strong>关键防护</strong>： <ul><li>在 <code>kettle.properties</code> 中<strong>强制指定连接池类型</strong>，避免泄漏：<div class="language-properties line-numbers-mode" data-highlighter="prismjs" data-ext="properties"><pre><code class="language-properties"><span class="line"><span class="token comment"># 启用防泄漏连接池</span></span>
<span class="line"><span class="token key attr-name">KETTLE_DATABASE_CONNECTION_POOL_TYPE</span><span class="token punctuation">=</span><span class="token value attr-value">HikariCP</span></span>
<span class="line"><span class="token key attr-name">KETTLE_DATABASE_HIKARI_LEAK_DETECTION_THRESHOLD</span><span class="token punctuation">=</span><span class="token value attr-value">5000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li></ol><blockquote><p>任何组件缺失导致的报错，可通过查看日志中的 <code>Caused by:</code> 快速定位——若含 <code>ClassNotFoundException: org.pentaho.hadoop.shims...</code> 则为 Shims 问题，否则检查 <code>lib</code> 目录驱动。</p></blockquote>`,28)])])}const i=e(o,[["render",r]]),c=JSON.parse('{"path":"/etl/kettle_select.html","title":"","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1749401044000,"contributors":[{"name":"Hankin","username":"Hankin","email":"hankin@catmes.com","commits":2,"url":"https://github.com/Hankin"}],"changelog":[{"hash":"dd122c34b3ed4c7c0eea5a931c6af4e579de3bc7","time":1749401044000,"email":"hankin@catmes.com","author":"Hankin","message":"UPDATE kettle"},{"hash":"48b649439b9c25ed4186ba9a4c120eb4b6e5bc18","time":1749394262000,"email":"hankin@catmes.com","author":"Hankin","message":"UPDATE kettle"}]},"filePathRelative":"etl/kettle_select.md"}');export{i as comp,c as data};
